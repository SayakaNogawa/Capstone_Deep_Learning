{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. \n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data I generated in build_a_libe_camera_1.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (228062, 32, 32, 1) (228062, 6)\n",
      "Validation set (7693, 32, 32, 1) (7693, 6)\n",
      "Test set (13068, 32, 32, 1) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'SVHN.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  data = pickle.load(f)\n",
    "  train_dataset = data['train_dataset']\n",
    "  train_labels = data['train_labels']\n",
    "  valid_dataset = data['valid_dataset']\n",
    "  valid_labels = data['valid_labels']\n",
    "  test_dataset = data['test_dataset']\n",
    "  test_labels = data['test_labels']\n",
    "  del data # To help gc free up memory\n",
    "  print ('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print ('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print ('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (228062, 1024) (228062, 6)\n",
      "Validation set (7693, 1024) (7693, 6)\n",
      "Test set (13068, 1024) (13068, 6)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "num_labels = 6\n",
    "\n",
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  return dataset\n",
    "\n",
    "train_dataset = reformat(train_dataset)\n",
    "valid_dataset = reformat(valid_dataset)\n",
    "test_dataset = reformat(test_dataset)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) \n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 32\n",
    "num_labels = 6\n",
    "beta = 0.001\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "num_units1 = 32*32*num_filters3\n",
    "num_units2 = 1024\n",
    "\n",
    "keep_prob = 0.5\n",
    "\n",
    "beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-204f10781bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Training computation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;31m# Optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayaka/.pyenv/versions/anaconda2-4.0.0/envs/tensorenv/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(logits, labels, name)\u001b[0m\n\u001b[1;32m    558\u001b[0m       raise ValueError(\"Rank mismatch: Labels rank (received %s) should equal \"\n\u001b[1;32m    559\u001b[0m                        \u001b[0;34m\"logits rank (received %s) - 1.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                        labels_static_shape.ndims, logits.get_shape().ndims)\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;31m# Check if no reshapes are required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_dataset_image = tf.reshape(tf_train_dataset, [-1,32,32,1])\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # conv & pool 1\n",
    "  num_filters1 = 32\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal([5,5,1, num_filters1], stddev=0.1))\n",
    "  h_conv1 = tf.nn.conv2d(tf_train_dataset_image, W_conv1, strides=[1,1,1,1], padding='SAME')\n",
    "  b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "  h_conv1_cutoff = tf.nn.relu(h_conv1+ b_conv1)\n",
    "  h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "  # conv & pool 2\n",
    "  num_filters2 = 64\n",
    "  W_conv2 = tf.Variable(tf.truncated_normal([5,5, num_filters1, num_filters2], stddev=0.1))\n",
    "  h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1,1,1,1], padding='SAME')\n",
    "  b_conv2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "  h_conv2_cutoff = tf.nn.relu(h_conv2+ b_conv2)\n",
    "  h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "  # conv & pool 3\n",
    "  num_filters3 = 64\n",
    "  W_conv3 = tf.Variable(tf.truncated_normal([5,5, num_filters2, num_filters3], stddev=0.1))\n",
    "  h_conv3 = tf.nn.conv2d(h_pool2, W_conv3, strides=[1,1,1,1], padding='SAME')\n",
    "  b_conv3 = tf.Variable(tf.constant(0.1, shape=[num_filters3]))\n",
    "  h_conv3_cutoff = tf.nn.relu(h_conv3+ b_conv3)\n",
    "  h_pool3 = tf.nn.max_pool(h_conv3_cutoff, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "  # Dropout.\n",
    "  h_pool3_flat = tf.reshape(h_pool3, [-1, 32*32*num_filters3])\n",
    "\n",
    "  W = tf.Variable(tf.truncated_normal([num_units1, num_units2], stddev=0.1))\n",
    "  b = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
    "  hidden = tf.nn.relu(tf.matmul(h_pool3_flat, W) + b)\n",
    "\n",
    "  hidden_dropout = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "  W0 = tf.Variable(tf.zeros([num_units2, num_labels]))\n",
    "  b0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  # Training computation.\n",
    "  logits =  tf.matmul(hidden_dropout, W0) + b0\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf_train_labels)) \n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_h1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_h1) + biases_h1)\n",
    "  train_h2 = tf.nn.relu(tf.matmul(train_h1, weights_h2) + biases_h2)\n",
    "  train_h3 = tf.nn.relu(tf.matmul(train_h2, weights_h3) + biases_h3)\n",
    "  train_h4 = tf.nn.relu(tf.matmul(train_h3, weights_h4) + biases_h4)\n",
    "  train_logits = tf.matmul(train_h4, weights_o) + biases_o\n",
    "  train_prediction = tf.nn.softmax(train_logits)\n",
    "  \n",
    "  valid_h1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_h1) + biases_h1)\n",
    "  valid_h2 = tf.nn.relu(tf.matmul(valid_h1, weights_h2) + biases_h2)\n",
    "  valid_h3 = tf.nn.relu(tf.matmul(valid_h2, weights_h3) + biases_h3)\n",
    "  valid_h4 = tf.nn.relu(tf.matmul(valid_h3, weights_h4) + biases_h4)\n",
    "  valid_logits = tf.matmul(valid_h4, weights_o) + biases_o\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "    \n",
    "  test_h1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_h1) + biases_h1)\n",
    "  test_h2 = tf.nn.relu(tf.matmul(test_h1, weights_h2) + biases_h2)\n",
    "  test_h3 = tf.nn.relu(tf.matmul(test_h2, weights_h3) + biases_h3)\n",
    "  test_h4 = tf.nn.relu(tf.matmul(test_h3, weights_h4) + biases_h4)\n",
    "  test_logits = tf.matmul(test_h4, weights_o) + biases_o\n",
    "  test_prediction = tf.nn.softmax(test_logits)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tensorflow.python.framework.ops.Operation object at 0x12a61f710> of <tensorflow.python.framework.ops.Operation object at 0x12a61f710> cannot be interpreted as a Tensor. (Operation name: \"Adam\"\nop: \"AssignAdd\"\ninput: \"Variable\"\ninput: \"Adam/value\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_INT32\n  }\n}\nattr {\n  key: \"_class\"\n  value {\n    list {\n      s: \"loc:@Variable\"\n    }\n  }\n}\nattr {\n  key: \"use_locking\"\n  value {\n    b: false\n  }\n}\n is not an element of this graph.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-068f9674ac20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf_train_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayaka/.pyenv/versions/anaconda2-4.0.0/envs/tensorenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayaka/.pyenv/versions/anaconda2-4.0.0/envs/tensorenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;31m# Validate and process fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0mprocessed_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0munique_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sayaka/.pyenv/versions/anaconda2-4.0.0/envs/tensorenv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_process_fetches\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n\u001b[0;32m--> 553\u001b[0;31m                            'Tensor. (%s)' % (subfetch, fetch, str(e)))\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m           raise ValueError('Fetch argument %r of %r cannot be interpreted as a '\n",
      "\u001b[0;31mValueError\u001b[0m: Fetch argument <tensorflow.python.framework.ops.Operation object at 0x12a61f710> of <tensorflow.python.framework.ops.Operation object at 0x12a61f710> cannot be interpreted as a Tensor. (Operation name: \"Adam\"\nop: \"AssignAdd\"\ninput: \"Variable\"\ninput: \"Adam/value\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_INT32\n  }\n}\nattr {\n  key: \"_class\"\n  value {\n    list {\n      s: \"loc:@Variable\"\n    }\n  }\n}\nattr {\n  key: \"use_locking\"\n  value {\n    b: false\n  }\n}\n is not an element of this graph.)"
     ]
    }
   ],
   "source": [
    "num_steps=10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset=(step*batch_size)%(train_dataset.shape[0]-batch_size)\n",
    "        \n",
    "        batch_data=train_dataset[offset:(offset+batch_size),:]\n",
    "        batch_labels=train_labels[offset:(offset+batch_size),:]\n",
    "        \n",
    "        feed_dict={tf_train_dataset:batch_data,tf_train_labels:batch_labels}\n",
    "        \n",
    "        _,l,predictions=session.run([optimizer,loss,train_prediction],feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "            \n",
    "        global_step+=1\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
